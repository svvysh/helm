package scaffold

import (
	"encoding/json"
	"fmt"
	"strings"

	"github.com/polarzero/helm/internal/config"
)

func readmeTemplate() string {
	lines := []string{
		"# Helm Specs Workspace",
		"",
		"This `specs/` directory contains your runnable specs plus the prompt templates Helm uses at runtime.",
		"",
		"## What was scaffolded",
		"",
		"- `implement.prompt-template.md` — worker prompt (edit to change tone or required outputs).",
		"- `review.prompt-template.md` — verifier prompt (enforces STATUS: ok|missing + remainingTasks JSON).",
		"- `specs-breakdown-guide.md` — guidance the split command feeds to Codex when breaking down large specs.",
		"- `spec-00-example/` — a minimal example spec showing the required files:",
		"  - `SPEC.md`, `acceptance-checklist.md`, `metadata.json`, `implementation-report.md`.",
		"",
		"## How to use",
		"",
		"1. Add or edit `spec-XX-*` folders under this directory.",
		"2. Run `helm` to open the TUI, or use direct commands:",
		"   - `helm run` — pick and execute a spec (uses the Go runner).",
		"   - `helm spec` — paste a big spec to split it using `specs-breakdown-guide.md`.",
		"   - `helm status` — view overall state (when implemented).",
		"3. Customize the prompt templates here as needed; Helm always reads them from disk.",
		"",
		"## File conventions",
		"",
		"- Each spec lives in `spec-XX-name/` with the four files above.",
		"- `metadata.json` tracks status (`todo`, `in-progress`, `done`) and dependencies.",
		"- `acceptance-checklist.md` plus your acceptance commands guide the verifier.",
		"",
		"Customize freely—these files are meant to be edited per-repo.",
	}
	return strings.Join(lines, "\n")
}

func specsBreakdownGuideTemplate() string {
	lines := []string{
		"# Spec Breakdown Guide",
		"",
		"Large specs should be divided into smaller, incremental `spec-XX-*` folders so that each attempt stays focused and verifiable. Use the following checklist when splitting work:",
		"",
		"1. **Identify independent threads.** Break the product spec into vertical slices (CLI command, backend API, docs updates, etc.). Each slice should deliver end-user value or clear internal infrastructure.",
		"2. **Define crisp acceptance criteria.** Every spec must include acceptance commands and a short checklist so the verifier can decide pass/fail without guesswork.",
		"3. **List dependencies explicitly.** If a spec requires another one to finish first, add its ID to the `dependsOn` array in `metadata.json`.",
		"4. **Keep scope tight.** Prefer many short specs over one monolithic document. Aim for work that can be implemented and verified in a single focused session.",
		"5. **Document follow-ups.** If you intentionally defer work, capture it in the next spec’s `metadata.json` notes to maintain traceability.",
		"",
		"### Suggested workflow",
		"",
		"1. Paste the raw product spec into a scratch pad.",
		"2. Highlight nouns (features, commands, toggles) and verbs (actions) to reveal natural slices.",
		"3. Create a new `spec-XX-*` folder per slice, incrementing the numeric prefix.",
		"4. For each folder:",
		"   - Write `SPEC.md` (summary, goals, acceptance criteria).",
		"   - Draft `acceptance-checklist.md` with concrete checks.",
		"   - Initialize `metadata.json` with status \"todo\", dependencies, and acceptance commands.",
		"   - Add a placeholder `implementation-report.md`.",
		"When in doubt, split aggressively. Smaller specs reduce context for both humans and AI copilots, speeding up implementation and review.",
	}
	return strings.Join(lines, "\n")
}

func buildImplementPrompt(mode config.Mode) string {
	strictDetail := strings.Join([]string{
		"- You must not consider the spec \"done\" unless all acceptance commands succeed.",
		"- If commands fail for unrelated reasons, document them and leave the repo as clean as possible.",
	}, "\n")
	parallelDetail := strings.Join([]string{
		"- Focus on the files and packages touched by this spec.",
		"- Do not attempt repo-wide fixups outside the scope of the current spec.",
		"- Document unrelated failures without trying to fix them.",
	}, "\n")
	modeDetail := strictDetail
	if mode == config.ModeParallel {
		modeDetail = parallelDetail
	}

	template := strings.Join([]string{
		"# Implementation Prompt — {{SPEC_ID}}: {{SPEC_NAME}}",
		"",
		"You are an autonomous implementation agent working in this repository. Implement the spec below without changing unrelated parts of the codebase.",
		"",
		"Your current task is to implement **one incremental spec** within a larger roadmap. You must obey the spec boundaries and acceptance criteria for:",
		"",
		"- Spec ID: `{{SPEC_ID}}`",
		"- Spec name: `{{SPEC_NAME}}`",
		"- Mode: **{{MODE}}**",
		"",
		"---",
		"",
		"## Spec Text",
		"",
		"Use the following spec as the **single source of truth** for what must be implemented:",
		"",
		"```markdown",
		"{{SPEC_BODY}}",
		"```",
		"",
		"If you see contradictions or ambiguities, call them out in your report and make the smallest reasonable assumption to move forward.",
		"",
		"---",
		"",
		"## Acceptance Commands ({{MODE}} Mode)",
		"",
		"The following commands are required to pass in a clean run at the end of your work:",
		"",
		"{{ACCEPTANCE_COMMANDS}}",
		"",
		"%s",
		"",
		"---",
		"",
		"## Remaining Tasks from Previous Attempts",
		"",
		"You may be in the middle of an iterative loop. The verifier may have already identified remaining tasks:",
		"",
		"```json",
		"{{PREVIOUS_REMAINING_TASKS}}",
		"```",
		"",
		"Treat these as **high-priority TODOs** that must be addressed before you can consider the spec done.",
		"",
		"---",
		"",
		"## Required Deliverables",
		"",
		"In your final output, you MUST provide all of the following sections, in this order:",
		"",
		"1. `SUMMARY` — 3–7 bullet points describing what you implemented and why.",
		"2. `CHANGELOG` — bullet list of files you touched and the change made.",
		"3. `TRACEABILITY` — map each acceptance criterion to evidence in the code.",
		"4. `RUNBOOK` — steps to run the CLI feature plus the acceptance commands.",
		"5. `MANUAL SMOKE TEST` — human-verifiable steps with expected outcomes.",
		"6. `OPEN ISSUES & RISKS` — known gaps, follow-ups, or blocked work.",
		"",
		"Refer back to the spec frequently and keep your output concise and actionable.",
	}, "\n")
	return fmt.Sprintf(template, modeDetail)
}

func buildReviewPrompt() string {
	lines := []string{
		"# Verifier Prompt — {{SPEC_ID}}: {{SPEC_NAME}}",
		"",
		"You are a strict, read-only verifier. Validate only what the spec requires; do not request extra work outside the spec boundaries.",
		"",
		"Your job is to review the implementation agent’s work for the following spec:",
		"",
		"- Spec ID: `{{SPEC_ID}}`",
		"- Spec name: `{{SPEC_NAME}}`",
		"- Mode: **{{MODE}}**",
		"",
		"You have access to:",
		"- The spec text.",
		"- The implementation agent's latest report.",
		"- The acceptance checklist for this spec.",
		"",
		"You MUST NOT modify any files. You only inspect and reason.",
		"",
		"---",
		"",
		"## Inputs",
		"",
		"### Spec Text",
		"",
		"```markdown",
		"{{SPEC_BODY}}",
		"```",
		"",
		"### Acceptance Checklist (Human-Facing)",
		"",
		"```markdown",
		"{{ACCEPTANCE_CHECKLIST}}",
		"```",
		"",
		"### Required Acceptance Commands",
		"",
		"{{ACCEPTANCE_COMMANDS}}",
		"",
		"### Implementation Report",
		"",
		"```markdown",
		"{{IMPLEMENTATION_REPORT}}",
		"```",
		"",
		"---",
		"",
		"## Output Format (STRICT)",
		"",
		"You MUST follow this exact format:",
		"",
		"1. **First line**: overall status, exactly one of:",
		"   - `STATUS: ok`",
		"   - `STATUS: missing`",
		"2. **Second line**: JSON describing remaining tasks when status is \"missing\".",
		"   - Example: `{\"remainingTasks\":[\"write tests for X\"]}`",
		"   - If status is \"ok\", you may still output `{\"remainingTasks\":[]}`",
		"3. **Subsequent lines**: free-form Markdown commentary elaborating on your reasoning.",
		"",
		"If you do not follow this format, the runner will fail.",
		"",
		"---",
		"",
		"## Review Criteria",
		"",
		"1. **Spec Coverage** — Did the implementation satisfy every explicit requirement in the spec?",
		"2. **Acceptance Checklist** — Is there evidence that each checklist item is addressed?",
		"3. **Acceptance Commands** — Were the required commands run and reported?",
		"4. **Implementation Quality** — Are there correctness issues or missing wiring?",
		"5. **Honesty about Partial Work** — Missing deliverables must be captured as remaining tasks.",
		"",
		"Be conservative. If in doubt, prefer `STATUS: missing` with clear remaining tasks.",
	}
	return strings.Join(lines, "\n")
}

func exampleSpecBody() string {
	lines := []string{
		"# spec-00-example — Example Feature Spec",
		"",
		"## Summary",
		"",
		"Demonstrates how specs, metadata, and the implementation runner work together. This example does not change production code; it simply proves the workflow end to end.",
		"",
		"## Goals",
		"",
		"- Provide a concrete `spec-XX-*` folder that new contributors can inspect.",
		"- Exercise the implementation runner and verifier loop.",
		"- Document how acceptance commands and metadata fit together.",
		"",
		"## Non-Goals",
		"",
		"- No new CLI commands are shipped as part of this example.",
		"- Does not replace real specs for future roadmap items.",
		"",
		"## Detailed Requirements",
		"",
		"1. Keep this folder checked into source control so `helm scaffold` can recreate it elsewhere.",
		"2. Show how to structure `SPEC.md`, `acceptance-checklist.md`, `metadata.json`, and `implementation-report.md`.",
		"3. Reference the default acceptance commands captured in the global settings file (`~/.helm/settings.json`).",
		"4. Explain what the verifier should look for when reviewing future specs.",
		"",
		"## Acceptance Criteria",
		"",
		"- The acceptance checklist references each required command with short descriptions.",
		"- `metadata.json.status` starts as `\"todo\"` with no dependencies.",
		"- `implementation-report.md` documents when the runner last updated the spec (initially a placeholder).",
		"",
		"Use this example as a template for creating your own specs.",
	}
	return strings.Join(lines, "\n")
}

func exampleAcceptanceChecklist(commands []string) string {
	var b strings.Builder
	b.WriteString("# Acceptance Checklist — spec-00-example\n\n")
	b.WriteString("## Required Commands\n\n")
	for _, cmd := range commands {
		fmt.Fprintf(&b, "- `%s` — run after implementation to ensure regressions are caught.\n", cmd)
	}
	if len(commands) == 0 {
		b.WriteString("- (no commands configured)\n")
	}
	b.WriteString("\n## Manual Review\n\n")
	b.WriteString("- [ ] The example spec files remain intact and informative.\n")
	b.WriteString("- [ ] Metadata status reflects the latest verifier run.\n")
	return b.String()
}

func exampleImplementationReport() string {
	lines := []string{
		"# Implementation Report — spec-00-example",
		"",
		"This file will be overwritten by `implement-spec.mjs` after the first worker/verifier loop completes.",
		"",
		"- Mode: strict",
		"- Last status: not yet run",
		"- Remaining tasks: initialize the runner",
	}
	return strings.Join(lines, "\n")
}

//nolint:unused // kept to illustrate dependency graph seeds in scaffold docs.
func sampleDependencyGraph() string {
	graph := map[string]any{
		"specs": []map[string]any{
			{
				"id":        "spec-00-example",
				"name":      "Example spec workspace walkthrough",
				"dependsOn": []string{},
			},
			{
				"id":        "spec-01-follow-up",
				"name":      "Follow-up example feature",
				"dependsOn": []string{"spec-00-example"},
			},
		},
	}
	data, _ := json.MarshalIndent(graph, "", "  ")
	return string(data)
}

//nolint:unused // retained for documentation until the JS runner template returns.
func implementRunnerScript() string {
	lines := []string{
		"#!/usr/bin/env node",
		"import fs from 'fs/promises';",
		"import path from 'path';",
		"import { spawn } from 'child_process';",
		"",
		"async function main() {",
		"  const argv = process.argv.slice(2);",
		"  if (!argv[0]) {",
		"    console.error('Usage: implement-spec.mjs <spec-dir>');",
		"    process.exit(1);",
		"  }",
		"",
		"  const specArg = argv[0];",
		"  const cwd = process.cwd();",
		"",
		"  let specDirCandidate;",
		"  if (path.isAbsolute(specArg)) {",
		"    specDirCandidate = specArg;",
		"  } else {",
		"    specDirCandidate = path.resolve(cwd, specArg);",
		"  }",
		"",
		"  try {",
		"    await fs.access(specDirCandidate);",
		"  } catch {",
		"    const alt = path.resolve(cwd, 'docs', 'specs', specArg);",
		"    try {",
		"      await fs.access(alt);",
		"      specDirCandidate = alt;",
		"    } catch {",
		"      console.error('Could not find spec directory at', specDirCandidate, 'or', alt);",
		"      process.exit(1);",
		"    }",
		"  }",
		"",
		"  const specDir = specDirCandidate;",
		"  const specsRoot = path.dirname(specDir);",
		"",
		"  const metaPath = path.join(specDir, 'metadata.json');",
		"  const specMdPath = path.join(specDir, 'SPEC.md');",
		"  const checklistPath = path.join(specDir, 'acceptance-checklist.md');",
		"  const implTemplatePath = path.join(specsRoot, 'implement.prompt-template.md');",
		"  const reviewTemplatePath = path.join(specsRoot, 'review.prompt-template.md');",
		"  const settingsPath = path.join(process.env.HELM_CONFIG_DIR || path.join(process.env.HOME || '.', '.helm'), 'settings.json');",
		"  const reportPath = path.join(specDir, 'implementation-report.md');",
		"",
		"  let meta;",
		"  try {",
		"    meta = JSON.parse(await fs.readFile(metaPath, 'utf8'));",
		"  } catch (err) {",
		"    console.error('Failed to read metadata.json:', err);",
		"    process.exit(1);",
		"  }",
		"",
		"  let specBody;",
		"  try {",
		"    specBody = await fs.readFile(specMdPath, 'utf8');",
		"  } catch (err) {",
		"    console.error('Failed to read SPEC.md:', err);",
		"    process.exit(1);",
		"  }",
		"",
		"  let checklist = '';",
		"  try {",
		"    checklist = await fs.readFile(checklistPath, 'utf8');",
		"  } catch {",
		"    // optional",
		"  }",
		"",
		"  let implTpl, reviewTpl;",
		"  try {",
		"    implTpl = await fs.readFile(implTemplatePath, 'utf8');",
		"    reviewTpl = await fs.readFile(reviewTemplatePath, 'utf8');",
		"  } catch (err) {",
		"    console.error('Failed to read prompt templates:', err);",
		"    process.exit(1);",
		"  }",
		"",
		"  let settings = {};",
		"  try {",
		"    const raw = await fs.readFile(settingsPath, 'utf8');",
		"    settings = JSON.parse(raw);",
		"  } catch {",
		"    settings = {};",
		"  }",
		"",
		"  const mode = settings.mode || 'strict';",
		"  const acceptanceCommands = meta.acceptanceCommands || settings.acceptanceCommands || [];",
		"  const acceptanceCommandsText = acceptanceCommands.length",
		"    ? acceptanceCommands.map(c => `- ${c}`).join('\\n')",
		"    : '- (none specified)';",
		"",
		"  const maxAttempts = parseInt(process.env.MAX_ATTEMPTS || settings.defaultMaxAttempts || '2', 10);",
		"  const modelImpl = process.env.CODEX_MODEL_IMPL || settings.codexModelRunImpl || 'gpt-5.1-codex';",
		"  const modelVer = process.env.CODEX_MODEL_VER || settings.codexModelRunVer || 'gpt-5.1-codex';",
		"",
		"  const specID = meta.id || path.basename(specDir);",
		"  const specName = meta.name || extractTitle(specBody) || '(unnamed spec)';",
		"",
		"  let remainingTasks = [];",
		"",
		"  for (let attempt = 1; attempt <= maxAttempts; attempt++) {",
		"    const now = new Date();",
		"",
		"    console.log(`\\n=== Attempt ${attempt} of ${maxAttempts} for ${specID} ===\\n`);",
		"",
		"    const workerPrompt = implTpl",
		"      .replace(/{{SPEC_ID}}/g, specID)",
		"      .replace(/{{SPEC_NAME}}/g, specName)",
		"      .replace(/{{SPEC_BODY}}/g, specBody)",
		"      .replace(/{{ACCEPTANCE_COMMANDS}}/g, acceptanceCommandsText)",
		"      .replace(/{{PREVIOUS_REMAINING_TASKS}}/g, JSON.stringify(remainingTasks, null, 2))",
		"      .replace(/{{MODE}}/g, mode);",
		"",
		"    const workerOutput = await runCodex(workerPrompt, false, modelImpl);",
		"",
		"    const reviewInput = reviewTpl",
		"      .replace(/{{SPEC_ID}}/g, specID)",
		"      .replace(/{{SPEC_NAME}}/g, specName)",
		"      .replace(/{{SPEC_BODY}}/g, specBody)",
		"      .replace(/{{ACCEPTANCE_CHECKLIST}}/g, checklist)",
		"      .replace(/{{ACCEPTANCE_COMMANDS}}/g, acceptanceCommandsText)",
		"      .replace(/{{IMPLEMENTATION_REPORT}}/g, workerOutput)",
		"      .replace(/{{MODE}}/g, mode);",
		"",
		"    const verifierOutput = await runCodex(reviewInput, true, modelVer);",
		"",
		"    const lines = verifierOutput.split(/\\r?\\n/).filter(Boolean);",
		"    if (lines.length < 2) {",
		"      console.error('Verifier output did not contain at least two non-empty lines');",
		"      process.exit(1);",
		"    }",
		"",
		"    const statusLine = lines[0].trim();",
		"    const jsonLine = lines[1].trim();",
		"",
		"    let status;",
		"    if (statusLine === 'STATUS: ok') {",
		"      status = 'ok';",
		"    } else if (statusLine === 'STATUS: missing') {",
		"      status = 'missing';",
		"    } else {",
		"      console.error('Unexpected status line from verifier:', statusLine);",
		"      process.exit(1);",
		"    }",
		"",
		"    let json;",
		"    try {",
		"      json = JSON.parse(jsonLine);",
		"    } catch (err) {",
		"      console.error('Failed to parse verifier JSON line:', err);",
		"      process.exit(1);",
		"    }",
		"    remainingTasks = Array.isArray(json.remainingTasks) ? json.remainingTasks : [];",
		"",
		"    const notePrefix = `[${now.toISOString()}] attempt ${attempt} status=${status}`;",
		"    if (status === 'ok') {",
		"      meta.status = 'done';",
		"      meta.lastRun = now.toISOString();",
		"      meta.notes = (meta.notes || '') + `\\n${notePrefix}: ok`;",
		"    } else {",
		"      meta.status = 'in-progress';",
		"      meta.lastRun = now.toISOString();",
		"      meta.notes = (meta.notes || '') + `\\n${notePrefix}: remaining tasks: ${remainingTasks.join('; ')}`;",
		"    }",
		"",
		"    await fs.writeFile(metaPath, JSON.stringify(meta, null, 2), 'utf8');",
		"",
		"    const report = [",
		"      `# Implementation Report for ${specID} — ${specName}`.trim(),",
		"      '',",
		"      `- Mode: ${mode}`,",
		"      `- Max attempts: ${maxAttempts}`,",
		"      `- Attempts used: ${attempt}`,",
		"      `- Final verifier status: ${status}`,",
		"      '',",
		"      '## Remaining tasks',",
		"      '',",
		"      JSON.stringify({ remainingTasks }, null, 2),",
		"      '',",
		"      '## Final worker output',",
		"      '',",
		"      workerOutput",
		"    ].join('\\n');",
		"",
		"    await fs.writeFile(reportPath, report, 'utf8');",
		"",
		"    if (status === 'ok') {",
		"      process.exit(0);",
		"    }",
		"",
		"    if (attempt < maxAttempts) {",
		"      console.log('\\nVerifier reported remaining tasks; continuing to next attempt.\\n');",
		"    }",
		"  }",
		"",
		"  console.error(`Exhausted ${maxAttempts} attempts without STATUS: ok.`);",
		"  process.exit(1);",
		"}",
		"",
		"function extractTitle(markdown) {",
		"  const lines = markdown.split(/\\r?\\n/);",
		"  for (const line of lines) {",
		"    const trimmed = line.trim();",
		"    if (trimmed.startsWith('# ')) {",
		"      return trimmed.replace(/^#\\s+/, '');",
		"    }",
		"  }",
		"  return null;",
		"}",
		"",
		"async function runCodex(prompt, readOnly, model) {",
		"  return new Promise((resolve, reject) => {",
		"    const args = ['exec'];",
		"    if (readOnly) {",
		"      args.push('--sandbox', 'read-only');",
		"    } else {",
		"      args.push('--dangerously-bypass-approvals-and-sandbox');",
		"    }",
		"    args.push('--model', model);",
		"",
		"    const child = spawn('codex', args, { stdio: ['pipe', 'pipe', 'pipe'] });",
		"",
		"    let output = '';",
		"    child.stdout.on('data', chunk => {",
		"      const text = chunk.toString();",
		"      process.stdout.write(text);",
		"      output += text;",
		"    });",
		"    child.stderr.on('data', chunk => {",
		"      const text = chunk.toString();",
		"      process.stderr.write(text);",
		"    });",
		"    child.on('error', err => reject(err));",
		"    child.on('close', code => {",
		"      if (code !== 0) {",
		"        reject(new Error(`codex exited with code ${code}`));",
		"      } else {",
		"        resolve(output);",
		"      }",
		"    });",
		"",
		"    child.stdin.write(prompt);",
		"    child.stdin.end();",
		"  });",
		"}",
		"",
		"main().catch(err => {",
		"  console.error('Unexpected error:', err);",
		"  process.exit(1);",
		"});",
	}
	return strings.Join(lines, "\n")
}
